\documentclass[11pt]{article}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}

% link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}

\definecolor{mygray}{RGB}{235,235,235}

\setlength{\topmargin}{-2cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{24cm}
\setlength{\evensidemargin}{0cm}
\setlength{\oddsidemargin}{0cm}

\lstset{basicstyle=\ttfamily, keywordstyle=\color{blue}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  commentstyle=\color{magenta}\ttfamily,
  morecomment=[l][\color{magenta}]{\#}}

\lstnewenvironment{cli}
               {\footnotesize \lstset{columns=flexible,language=bash,backgroundcolor=\color{mygray}}}
               {}

\lstnewenvironment{xml}
               {\footnotesize \lstset{columns=flexible,language=xml,backgroundcolor=\color{Salmon}}}
               {}

               
\lstnewenvironment{bash}
               {\footnotesize \lstset{columns=flexible,language=bash,backgroundcolor=\color{SkyBlue}}}
               {}
               
\lstnewenvironment{java}
               {\lstset{language=java,columns=flexible,backgroundcolor=\color{YellowGreen}}}
               {}

               
\newcommand{\postit}[1]{%
  \noindent
  \fcolorbox{red}{yellow}{%
    \begin{minipage}{5cm}
      #1
    \end{minipage}
   }
}


\title{\textsc{Big Data Analytics (SOEN 498/691)} \\ Laboratory sessions}

\author{Tristan Glatard\\Department of Computer Science and Software Engineering\\Concordia University, Montreal\\\href{mailto:tristan.glatard@concordia.ca}{tristan.glatard@concordia.ca}}


\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\part{Prerequisites}

\section{Notations}

This is a command you are supposed to type in a Linux terminal (don't type the \$ sign):
\begin{cli}
  $ echo Welcome
\end{cli}
This command is split in two lines:
\begin{cli}
  $ echo Welcome to this tutorial,\
    I hope you will enjoy it! 
\end{cli}
This is a piece of Java code:
\begin{java}
  int a=7;
\end{java}
This is a piece of bash script:
\begin{bash}
  #!/bin/bash
  for i in 1 2 3
  do
    echo $i
  done
\end{bash}
And this is a piece of XML file:
\begin{xml}
  <one>
    <two>Hello</two>
  </one>
\end{xml}

\section{Java}

Java is the primary language used in Hadoop. Make sure that Java
version 7 or higher is installed on your system. If it is not, follow
the instructions from
\href{http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html}{the
  Oracle website} and check that the Java Virtual Machine (JVM) works correctly:
\begin{cli}
$ java -version
\end{cli}
Make sure that you are able to create and run a simple Java
such as:
\begin{java}
public class Test{
  public static void main(String[] args){
    System.out.println(''Hello, World!'');
  }
}
\end{java}
This program is compiled as follows:
\begin{cli}
$ javac Test.java
\end{cli}
The compilation produces a \texttt{Test.class} file that contains the compiled class.
The program can be executed as follows:
\begin{cli}
$ java Test
\end{cli}
\texttt{Test.class} must be located in the directory where \texttt{java} is
executed, or in a directory listed in the \texttt{CLASSPATH}
environment variable.

This tutorial can be done with Java command-line tools
(\texttt{javac} and \texttt{java}) a text editor such as \texttt{vim}
or \texttt{emacs}. For larger applications, for instance your course
project, it is recommended to use an Integrated Development
Environment (IDE) tailored for Java, e.g.,
\href{http://eclipse.org}{Eclipse} or
\href{http://netbeans.org}{Netbeans}.

\section{Linux}
This tutorial is based on Linux and it uses the
\href{https://www.gnu.org/software/bash/}{bash} shell. You don't have
to install bash as it is already present on your workstation.

\subsection{Useful commands}
\begin{itemize}
\item \texttt{cat}: prints the content of a file on the standard output.
\item \texttt{chmod}: changes the access permissions of a file or directory.
\item \texttt{echo}: prints a message on the standard output.
\item \texttt{export}: exports environment variables to the
  environment of subsequently executed commands.
\item \texttt{ls}: lists directory content.
\item \texttt{man}: gets help on any command.
\item \texttt{mkdir}: creates a directory.
\item \texttt{seq 1 1000}: prints the integers from 1 to 1000. 
\item \texttt{tar}: handles 
  \texttt{tar}, \texttt{tgz} and many other types of archives.
\item \texttt{wget}: downloads content of a URL.
\end{itemize}

\subsection{Standard streams}

A Linux process is connected to three particular files called \emph{streams}.
\begin{itemize}
\item The standard \emph{output} is used by the process to write normal information.
\item The standard \emph{error} is used by the process to write error information.
\item The standard \emph{input} is used to send information to the process.
\end{itemize}
We will use streams extensively in the tutorial.

\subsection{Useful operators}
\begin{itemize}
  \item \texttt{>} (redirection operator): redirects the standard
    output of its left argument to a file. For instance, \texttt{echo
      Hello > a.txt} writes ``Hello'' in file \texttt{a.txt} (if
    \texttt{a.txt} already exists, it is overwritten).
    \item \texttt{>>}: same as \texttt{>} but the standard output of
      the left operand is \emph{appended} to the file in the right operand.
  \item \texttt{|} (pipe operator): redirects the standard output of
    its left argument to the standard input of its right argument. For
    instance, \texttt{cat a.txt | sort} prints the sorted content of a
    file.
\end{itemize}
Here is also a list of useful environment variables:
\begin{itemize}
  \item \texttt{PATH}: tells the system which directories to search for executables.
  \item \texttt{CLASSPATH}: tells the JVM where classes must be searched.
\end{itemize}

\newpage

\part{Getting started with Hadoop}

This part of the tutorial is adapted from the following sources:
\begin{itemize}
\item \href{http://hadoop.apache.org/docs/r2.7.3/index.html}{Apache Hadoop documentation}
\item \href{http://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html}{MapReduce tutorial}
\item \href{http://hadoopbook.com}{Tom White, ``Hadoop: The Definitive Guide''. Chapter 2 (MapReduce), Chapter 3 (HDFS).}
\end{itemize}

\section{Standalone mode}

\subsection{Installation}

Download Hadoop release 2.7.3 from one of the
\href{http://www.apache.org/dyn/closer.cgi/hadoop/common/}{Apache
  Download Mirrors} and unpack the distribution:
\begin{cli}
  $ wget http://apache.mirror.rafal.ca/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
  $ tar xf hadoop-2.7.3.tar.gz
  $ export HADOOP=$PWD/hadoop-2.7.3
\end{cli}
Edit \texttt{etc/hadoop/hadoop-env.sh} and define the path to your Java installation:
\begin{cli}
  export JAVA_HOME=/path/to/your/java/installation/
\end{cli}
Add the Hadoop executables to your PATH:
\begin{cli}
  $ export PATH=$PATH:${HADOOP}/bin:${HADOOP}/sbin
\end{cli}
Make sure Hadoop is installed properly:
\begin{cli}
  $ hadoop version
\end{cli}
Run a first example:
\begin{cli}
  $ mkdir input
  # creates a test file containing integers from 1 to 1000
  $ for i in `seq 1 1000`; do echo $i >> input/file.txt ; done 
  $ hadoop jar ${HADOOP}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 1
  $ cat output/*
\end{cli}
This program prints the lines of the input file that contain ``1''.

\subsection{First MapReduce program}

As a first MapReduce program, write the classical WordCount example:
\begin{java}
  import java.io.IOException;
  import java.util.StringTokenizer;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.Reducer;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

  public class WordCount {

    // The class that will be used for the map tasks. Has to extend ''Mapper''.
    // In general, overrides the ``map'' method.
    // < > denotes the use of a Generic Type.  Text and IntWritable are Hadoop
    // classes used for keys and values. In Hadoop, any key or value has to
    // implement specific interfaces (Java's String or Integer cannot
    // be used directly). 
    public static class TokenizerMapper
    extends Mapper<Object, Text, Text, IntWritable>{

      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();

      // The method that will be used for the map tasks.
      // Context is a class defined in Mapper, that gives access to
      // input and output key/value pairs.
      public void map(Object key, Text value, Context context
      ) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens()) {
          word.set(itr.nextToken());
          // Emits key/value pair in the form <word, 1>
          context.write(word, one);
        }
      }
    }

    // The class that will be used for the reducers and combiners.
    public static class IntSumReducer
    extends Reducer<Text,IntWritable,Text,IntWritable> {
      private IntWritable result = new IntWritable();
      
      public void reduce(Text key, Iterable<IntWritable> values,
      Context context
      ) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
          sum += val.get();
        }
        result.set(sum);
        context.write(key, result);
      }
    }

    // Main method
    public static void main(String[] args) throws Exception {
      // Configuration provides access to configuration parameters
      Configuration conf = new Configuration();
      // Create a new Hadoop job named ``word count''
      Job job = Job.getInstance(conf, "word count");
      // Sets the Jar by finding where the WordCount class came from
      job.setJarByClass(WordCount.class);
      // Sets mapper class for the job.
      job.setMapperClass(TokenizerMapper.class);
      // Sets combiner class for the job.
      job.setCombinerClass(IntSumReducer.class);
      // Sets reducer class for the job.
      job.setReducerClass(IntSumReducer.class);
      // Set the key class for the job output data.
      job.setOutputKeyClass(Text.class);
      // Set the value class for job outputs.
      job.setOutputValueClass(IntWritable.class);
      // Add a Path to the list of inputs for the map-reduce job.
      FileInputFormat.addInputPath(job, new Path(args[0]));
      // Set the Path of the output directory for the map-reduce job.
      FileOutputFormat.setOutputPath(job, new Path(args[1]));
      System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
  }
\end{java}
Add Hadoop's jars to the CLASSPATH:
\begin{cli}
  export CLASSPATH=$CLASSPATH:${HADOOP}/share/hadoop/common/hadoop-common-2.7.3.jar:\
             ${HADOOP}/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:.
\end{cli}
Compile the program:
\begin{cli}
$ javac WordCount.java
\end{cli}
Create a jar containing all the classes in WordCount:
\begin{cli}
  $ jar cvf wordcount.jar WordCount*.class
\end{cli}
Create test data and run the program:
\begin{cli}
  $ echo one two three two three three > test.txt
  $ hadoop jar wordcount.jar WordCount test.txt output
\end{cli}

\subsection{Hadoop Streaming}
\label{sec:hadoop-streaming}

While the Hadoop framework is developed in Java, it can also execute
programs written in any language using the Hadoop Streaming tool. In
this section, we are going to implement the WordCount example in
bash. You may also implement WordCount in any other language (Python,
Ruby, etc) if you wish.

Hadoop Streaming works as follows:
\begin{enumerate}
\item The key/value pairs are passed to the mapper executable through its standard input.
\item The mapper executable writes output key/value pairs to its standard output.
\item The sorted key/value pairs are passed to the reduce executable
  through its standard input.
\end{enumerate}

Hence, mappers and reducers can be tested as follows for any MapReduce
program executed with Hadoop Streaming:
\begin{cli}
  $ cat input.txt | mapper.sh | sort | reduce.sh
\end{cli}
The mapper code is straightforward:
\begin{bash}
  #!/bin/bash

  # Read all the lines passed on stdin
  while read line
  do
    # Split the words in the line
    for word in $line
    do
      # Emit a <word,1> key/value pair for every word
      echo $word 1
    done
  done
\end{bash}
The reducer code is a bit more complex:
\begin{bash}
  #!/bin/bash

  count=0
  while read line
  do
    # Declare an array containing all the elements on the line
    # About bash arrays: http://www.tldp.org/LDP/abs/html/arrays.html
    tokens=( $line )
    # The word (key) is the first element in the array
    new_word=${tokens[0]}
    # The number of occurrence of this word (value) is the second element in the array
    increment=${tokens[1]}
    # Remember that the key/value pairs produced by mappers
    # are sorted before being passed to the reducer.
    # Here we detect if the word in the key/value pair is the same
    # as at the previous iteration or if it has changed.
    if [[ ${new_word} = ${word} ]] || [ -z ${word+x} ]
    then
        # Word hasn't changed or it was not defined at the previous iteration:
        # increment the counter by the value in key/value pair.
	count=$((count+${increment}))
    else
        # Word has changed: emit the key/value pair and set the counter to the
        # value in key/value pair.
	echo $word $count
	count=${increment}
    fi
    # Save the word for next iteration    
    word=$new_word
done
# Emit last word
echo $word $count
\end{bash}
Run the program with Hadoop Streaming as follows:
\begin{cli}
  hadoop jar ${HADOOP}/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\
  -input test.txt\
  -output output\
  -mapper ./mapper.sh\
  -reducer ./reducer.sh\
  -file reducer.sh -file mapper.sh 
\end{cli}
Create a larger text file and run the program again. The number of
mappers and reducers can be changed with the following options:
\texttt{mapreduce.job.reduces} and \texttt{mapreduce.job.maps}. For
instance, to use two mappers and two reducers:
\begin{cli}
  hadoop jar ${HADOOP}/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\
  -D mapreduce.job.reduces=2\
  -D mapreduce.job.map=2\
  -input test.txt\
  -output output\
  -mapper ./mapper.sh\
  -reducer ./reducer.sh\
  -file reducer.sh -file mapper.sh 
\end{cli}
However, the framework uses only a single mapper and a single reducer
because it is configured in standalone mode, i.e., as a single Java
process. In the next section, we will configure the pseudo-distributed
mode where Hadoop runs in several processes.
%Funny enough, the number of mappers is ignored (because job tracker
%is set to local) while the number of reducers is not.

\section{Pseudo-distributed mode}


In standalone mode, Hadoop was using the local filesystem to store
files. In pseudo-distributed mode, it uses the Hadoop Distributed File
System (HDFS).

\subsection{ssh setup}

Make sure that you can connect to your own workstation using ssh
\emph{with no password}:
\begin{cli}
  $ ssh localhost
\end{cli}
If the ssh command prompts for a password, create a password-less ssh key and authorize it in your account:
\begin{cli}
  $ mkdir -p $HOME/.ssh
  $ chmod 700 $HOME/.ssh
  $ ssh-keygen
  # press 'Enter' at every question
  $ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
  $ chmod 600 $HOME/authorized_keys
\end{cli}

\subsection{HDFS}

\subsubsection{Configuration}

Configure the HDFS by adding the following to \texttt{etc/hadoop/core-site.xml}:
\begin{xml}
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/some/path/on/your/workstation</value>
    <description>A base for other temporary directories.</description>
  </property>
</configuration>
\end{xml}
and to \texttt{etc/hadoop/hdfs-site.xml}:
\begin{xml}
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
\end{xml}
Format the file system:
\begin{cli}
$  hdfs namenode -format
\end{cli}
And start the HDFS daemons:
\begin{cli}
$ start-dfs.sh
\end{cli}

\subsubsection{Command-line usage}

We will now review the main HDFS commands. Directory creation:
\begin{cli}
  $ hdfs dfs -mkdir /user
  $ hdfs dfs -mkdir /user/<username>
\end{cli}
Directory listing:
\begin{cli}
  $ hdfs dfs -ls /user
\end{cli}
File copy to HDFS:
\begin{cli}
  $ echo This is a test > test.txt
  $ hdfs dfs -put test.txt /user/<username>/test.txt
\end{cli}
File copy from HDFS:
\begin{cli}
  $ hdfs dfs -get /user/<username>/test.txt
\end{cli}
File content display:
\begin{cli}
  $ hdfs dfs -cat /user/<username>/test.txt
\end{cli}
File removal:
\begin{cli}
  $ hdfs dfs -rm /user/<username>/test.txt
\end{cli}

\subsubsection{Web interface}

HDFS can be browsed through a web interface which is by default
available at \href{http://localhost:50070}{http://localhost:50070}.

\subsubsection{Java API}

The \texttt{FileSystem} API is the safest way to access files on HDFS
from a Java program. A file in a Hadoop filsystem is represented by a
Hadoop \texttt{Path} object. You can think of a Path as a Hadoop
filesystem URI, such as
\texttt{hdfs://localhost/user/tom/quangle.txt}.

The following program reads a file on HDFS:
\begin{java}
import java.io.IOException;
import java.io.InputStream;
import java.net.URI;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;


public class HdfsCat {

    public static void main(String[] args) throws IOException {
        // URI to 'cat' will be passed as first argument
        String hdfsUri = args[0];
        // Create the FileSystem object
        FileSystem fs = FileSystem.get(URI.create(hdfsUri), new Configuration());
        // Create HDFS Path from URI
        Path path = new Path(hdfsUri);
        InputStream in = null;
        try{
            // Open an InputStream from the Path
            in = fs.open(path);
            // Copy bytes from InputStream to stdout
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally{
            // Close the InputStream
            IOUtils.closeStream(in);
        }
    }  
}
\end{java}
After having compiled this file, create a jar and run it with Hadoop:
\begin{cli}
  $ hadoop jar <path to jar> <HDFS URI of file to cat>
\end{cli}
A similar program can be written to write files to HDFS:
\begin{java}
import java.io.BufferedInputStream;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URI;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;


public class HdfsPut {
  public static void main(String[] args) throws FileNotFoundException, IOException{
      String localFilePath = args[0];
      String dest = args[1];
      InputStream in = new BufferedInputStream(new FileInputStream(localFilePath));
      FileSystem fs = FileSystem.get(URI.create(dest),new Configuration());
      OutputStream out = fs.create(new Path(dest));
      IOUtils.copyBytes(in, out, 4096, true);
  }
}
\end{java}

% distcp?

\subsection{YARN}

YARN (Yet Another Resource Negotiator) is the resource management
system used in pseudo-distributed mode.

\subsubsection{Configuration}

Edit \texttt{etc/hadoop/mapred-site.xml} to add the following configuration properties:
\begin{xml}
  <configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
  </configuration>
\end{xml}
Edit \texttt{etc/hadoop/yarn-site.xml} to add the following configuration properties:
\begin{xml}
  <configuration>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
      <description>Whether virtual memory limits will be enforced for containers.</description>
    </property>
  </configuration>
\end{xml}
Start the YARN daemon:
\begin{cli}
  $ start-yarn.sh
\end{cli}
The MapReduce job history daemon is another useful service to start:
\begin{cli}
  $ mr-jobhistory-daemon.sh start historyserver
\end{cli}

Re-run the Hadoop Streaming example of
Section~\ref{sec:hadoop-streaming} using multiple mappers and
reducers.

\subsubsection{Web interfaces}

Look at the YARN monitoring interface (\url{http://localhost:8088})
and the job history server (\url{http://localhost:19888}).

\section{More WordCount}


\end{document}


