\documentclass[11pt]{article}

% Packages
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}

% Colors
\definecolor{mygray}{RGB}{235,235,235}

% Margins
\setlength{\topmargin}{-2cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{24cm}
\setlength{\evensidemargin}{0cm}
\setlength{\oddsidemargin}{0cm}

% Fix link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}


% Code listings
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  commentstyle=\color{magenta}\ttfamily,
  morecomment=[l][\color{magenta}]{\#}
}


\lstnewenvironment{cli}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=bash,
                      backgroundcolor=\color{mygray}
                  }}
{}

\lstnewenvironment{xml}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=XML,
                      backgroundcolor=\color{Salmon},
                      morekeywords={property,name,value,description,configuration}
                  }}
{}

\newcommand{\bashcode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {
           \href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
                {(\underline{Link to file})
         }} \hfill
         \lstset{language=bash,
           columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\pythoncode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{YellowGreen}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{language=java,
           columns=fullflexible,
           backgroundcolor=\color{YellowGreen}}
%  \vspace*{-0.3cm}
%  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\textfile}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}


% Notes and TODOs              
\newcommand{\postit}[1]{%
  \noindent
  \fcolorbox{red}{yellow}{%
    \begin{minipage}{5cm}
      #1
    \end{minipage}
   }
}

\title{\textsc{Big Data Analytics (SOEN 498/691)} \\ Laboratory sessions}

\author{Tristan Glatard\\Department of Computer Science and Software Engineering\\Concordia University, Montreal\\\href{mailto:tristan.glatard@concordia.ca}{tristan.glatard@concordia.ca}}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\part{Apache Spark}

\section{Introduction}

\href{http://spark.apache.org}{Apache Spark} is steadily emerging as a
replacement of Hadoop MapReduce for the following reasons:
\begin{itemize}
\item Spark supports in-memory computing, which is faster
  than MapReduce's file-based model.
\item Spark's programming model is much richer than
  MapReduce.
  \item Spark can run on a variety of clusters, including but not
    limited to Hadoop.
\end{itemize}

The goal of this session is to install Apache Spark on your computer
and go through simple examples to understand its main
concepts. Although you won't have to submit anything at the end of
this session, the second lab assignment (LA2) will have to be
implemented using Spark so it is important that you complete this one
to the end.  Most of the material in this document is taken from the
Apache Spark online documentation:
\begin{itemize}
\item  \href{http://spark.apache.org/docs/latest/quick-start.html}{Quick Start Guide}
\item \href{http://spark.apache.org/docs/latest/programming-guide.html}{Programming Guide}
\end{itemize}
Feel free to explore this documentation further!

\section{Installation}

It is assumed that you already have a working Hadoop installation on
your computer. Download Apache Spark from
\href{http://spark.apache.org/downloads.html}{there}. Choose release
2.1.0, pre-built with user-provided Hadoop. You can also use the
command line directly:
\begin{cli}
  $ wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-without-hadoop.tgz
\end{cli}
Unpack the release and write the following lines to \texttt{conf/spark-env.sh}, as explained
\href{http://spark.apache.org/docs/latest/hadoop-provided.html}{here}:
\bashcode{spark/spark-env.sh}
Add Spark's \texttt{bin} directory to your \texttt{PATH} environment
variables so that the system can find Spark's commands:
\begin{cli}
  $ export PATH=$PATH:$PWD/spark-2.1.0-bin-without-hadoop/bin
\end{cli}
Make sure that the following example runs correctly:
\begin{cli}
  run-example SparkPi 10
\end{cli}
\texttt{run-example} is a program located in
\texttt{\$PWD/spark-2.1.0-bin-without-hadoop/bin}. In case the output
of the previous command line is \texttt{Command not found}, check your
\texttt{PATH}.

\section{Resilient Distributed Datasets (RDD)}

Spark programs can be written in Java, Scala, Python and R. Although
we use Python in the remainder of this document, feel free to use any
other language and find the corresponding commands in the
\href{http://spark.apache.org/docs/latest/programming-guide.html}{Programming
  Guide}.

An easy way to start using Spark is through \texttt{pyspark}, a
Spark Python shell started as follows:
\begin{cli}
  $ pyspark
\end{cli}
In the following, commands starting with \texttt{>>>} are typed in
\texttt{pyspark}.
Spark relies on the concept of Resilient Distributed Datasets
(RDDs). RDDs are collections of elements that can be processed in
parallel and created from regular data structures or
files. For instance, here is how to create a RDD from an array of integers:
\begin{cli}
  >>> d = [1,2,3,4]
  >>> pd = sc.parallelize(d)
\end{cli}
Here, \texttt{sc} is the Spark Context object provided by
\texttt{pyspark}. In a standalone Spark program, you can create \texttt{sc} as follows:
\pythoncode{spark/context.py}

In Spark, two types of operations can be performed on RDDs:
\emph{transformations} produce another RDD from a given RDD, and
\emph{actions} produce a simple value, e.g., an integer, from a given
RDD. The \texttt{map} function is an example of transformation used in
the following code to add 1 to all the elements in RDD \texttt{pd}:
\begin{cli}
  >>> inc=pd.map(lambda x: x+1)
\end{cli}
Here are a few remarks about this line of code:
\begin{enumerate}
\item Spark's \texttt{map} function is \underline{very different} from
  MapReduce's \texttt{map} function. Spark's
  \texttt{map} can be used to implement a MapReduce \texttt{map}
  function but it is much more general than that.
\item In Python, a \emph{lambda} is an anonymous function and the expression \texttt{lambda arguments:
    expression} yields a function object. See more details in the
  \href{https://docs.python.org/2/reference/expressions.html#lambda}{Python
    documentation}.  Here, a lambda is used to provide 
  the \texttt{map} function with a function that will be applied to all the elements
  in \texttt{pd}.
  \item At this stage, nothing has actually been computed by
    Spark. That is, \texttt{inc} only contains a reference to the
    result of the transformation that \emph{will} be applied to \texttt{pd}. The
    actual value will be computed as late as possible, when it is
    really needed. This is called \emph{lazy evaluation}. 
\end{enumerate}
Let's now use the \texttt{collect} action to return all the elements
in the RDD as a Python array:
\begin{cli}
  >>> result=inc.collect()
\end{cli}
The result of your first Spark program should now appear in the shell!

\section{WordCount}

We will now implement with Spark the classical WordCount example. We
will follow the same logic as with MapReduce, i.e., our program will
emit a \texttt{(w,1)} pair for every word found in the input text
file. Then it will sum the \texttt{1}s associated with a given word
\texttt{w}.

\subsection{Step-by-step presentation}

First, let's create a simple input text file:
\begin{cli}
  $ echo one two three two three three > /tmp/test.txt
  $ echo one two three >> /tmp/test.txt
\end{cli}
Our first step will be to create a RDD from the input text file:
\begin{cli}
  >>> lines = sc.textFile(``file:///tmp/test.txt'')
\end{cli}
This RDD contains one element for every line in the text file.  Now we
need to split those lines into words, which we will do using the
\texttt{flatMap} transformation.  \texttt{flatMap} is used when an
element in the input RDD is mapped to an arbitrary number of elements
(0 or more) in the output RDD. In our case, every line in the text
file will be mapped to all the words in this line:
\begin{cli}
  >>> words=lines.flatMap(lambda x: x.split())
\end{cli}
RDD \texttt{words} now contains all the words in the dataset (or it
will contain them when the program is evaluated, remember that Spark
is lazy). We will now convert these words to \texttt{(w,1)} pairs using a
\texttt{map} transformation:
\begin{cli}
  >>> pairs=words.map(lambda x: (x,1))
\end{cli}
Finally, pairs need to be ``reduced'', i.e., they need to be grouped
by their first elements and the corresponding \texttt{1}s must be
summed up. This is done using the \texttt{reduceByKey} transformation:
\begin{cli}
  >>> counts=pairs.reduceByKey(lambda x,y: x+y)
\end{cli}
The function passed to \texttt{reduceByKey} is a binary
operator that takes as argument two values from the key-value pairs
and returns one. Besides, this function must be
\underline{commutative} and \underline{associative}. 
The word counts can now be printed using the \texttt{foreach} transformation:
\begin{cli}
  >>> def g(x):
  ...   print x
  >>> counts.foreach(lambda x: g(str(x[0])+'': ''+str(x[1])))
\end{cli}
Or they might be saved in a text file using the \texttt{saveAsTextFile} action:
\begin{cli}
  >>> counts.saveAsTextFile(``file:///tmp/counts'')
\end{cli}

\subsection{Complete program}

Here is a complete version of WordCount in Spark:
\pythoncode{spark/wordcount.py}
To run this program, you will have to update your environment as follows:
\begin{cli}
  $ export PYTHONPATH=$PWD/spark-2.1.0-bin-without-hadoop/python
  $ sudo pip install py4j
\end{cli}
The program can be executed as follows:
\begin{cli}
  $ ./wordcount.py file:///tmp/test.txt file:///tmp/counts
\end{cli}
Spark can transparently work with files stored on HDFS. Start your
HDFS daemon, upload \texttt{test.txt} to HDFS and re-run the WordCount program:
\begin{cli}
  $ ./wordcount.py hdfs://localhost:9000/test.txt file:///tmp/counts-hdfs
\end{cli}

\section{Going further}

A complete list of transformations and actions available on RDDs is in
the
\href{http://spark.apache.org/docs/latest/programming-guide.html}{Programming
  Guide}. Examples are also available in Spark's
\href{https://github.com/apache/spark}{Github repository}. Try
re-implementing in Spark the kmeans clustering algorithm programmed in
our
\href{https://github.com/glatard/big-data-analytics-course/releases/download/0.7.3/kmeans.pdf}{previous
  lab session} using MapReduce. A solution is available
\href{https://github.com/apache/spark/tree/master/examples/src/main/python/kmeans.py}{there}.

\end{document}


