\documentclass[11pt]{article}

% Packages
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}

% Colors
\definecolor{mygray}{RGB}{235,235,235}

% Margins
\setlength{\topmargin}{-2cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{24cm}
\setlength{\evensidemargin}{0cm}
\setlength{\oddsidemargin}{0cm}

% Fix link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}


% Code listings
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  commentstyle=\color{magenta}\ttfamily,
  morecomment=[l][\color{magenta}]{\#}
}


\lstnewenvironment{cli}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=bash,
                      backgroundcolor=\color{mygray}
                  }}
{}

\lstnewenvironment{xml}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=XML,
                      backgroundcolor=\color{Salmon},
                      morekeywords={property,name,value,description,configuration}
                  }}
{}

\newcommand{\bashcode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {
           \href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
                {(\underline{Link to file})
         }} \hfill
         \lstset{language=bash,
           columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\pythoncode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{YellowGreen}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{language=java,
           columns=fullflexible,
           backgroundcolor=\color{YellowGreen}}
%  \vspace*{-0.3cm}
%  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\textfile}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}


% Notes and TODOs              
\newcommand{\postit}[1]{%
  \noindent
  \fcolorbox{red}{yellow}{%
    \begin{minipage}{5cm}
      #1
    \end{minipage}
   }
}

\title{\textsc{Big Data Analytics (SOEN 498/691)} \\ Laboratory sessions}

\author{Tristan Glatard\\Department of Computer Science and Software Engineering\\Concordia University, Montreal\\\href{mailto:tristan.glatard@concordia.ca}{tristan.glatard@concordia.ca}}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\part{MLlib}

\section{Introduction}

MLlib (Machine-Learning library) is a library in
\href{http://spark.apache.org}{Apache Spark} that contains
implementations of several algorithms seen in the lecture. In this
session, we will run through basic examples related to clustering,
recommendation systems and the mining of frequent itemsets. These
examples may help you start with your project. Most of the material is
taken from the
\href{http://spark.apache.org/docs/latest/ml-guide.html}{MLlib
  guide} and the \href{http://spark.apache.org/docs/latest/api/python/index.html}{Pyspark
  documentation}. You are encouraged to further explore these references should you
need more details about MLlib. For installation
instructions and an introduction to Apache Spark, please refer to the
\href{https://github.com/glatard/big-data-analytics-course/releases/download/0.8/spark.pdf}{previous
  lab session}. Warning: MLlib is still under active development and
backward compatibility even between minor versions is not
ensured. Here we use version 2.1.0.


\section{The DataFrame API}

This section is adapted from the
\href{http://spark.apache.org/docs/latest/sql-programming-guide.html}{Spark
  DataFrame Guide}. In the introduction session we presented how
Resilient Distributed Datasets (RDDs) are used to define parallel
operations on datasets. MLlib uses another kind of data objects,
called DataFrame. A DataFrame consists of named columns. It is
conceptually equivalent to a table in a database.

Before using the DataFrame API, a Spark session must be created:
\begin{cli}
  >>> spark = SparkSession.builder.appName("Lab session").getOrCreate()
\end{cli}
A DataFrame can be created from an RDD or from a file. For instance,
here is how to read a DataFrame from a
\href{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}{libSVM} file (in the
remainder of this document, it is assumed that environment variable
\texttt{SPARK\_HOME} points to your Spark installation):
\begin{cli}
  >>> import os
  >>> spark_home = os.environ['SPARK_HOME']
  >>> dataset = spark.read.format("libsvm").load(\
      "file://"+os.path.join(spark_home,"data/mllib/sample_kmeans_data.txt"))
\end{cli}
DataFrame objects can then be inspected and queried as a database relation:
\begin{cli}
>>> dataset.show()
>>> dataset.select("label").show()
>>> dataset.filter(dataset["label"]==1).show()
\end{cli}
See the \href{http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame}{PySpark documentation} for more details.

\section{Clustering}

MLlib includes an enhanced version of kmeans called
\href{http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf}{kmeans$\mid\mid$}. The
library is quite straightforward to use, but you should note that
KMeans are implemented as an MLlib
\href{http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=estimator#pyspark.ml.Estimator}{Estimator}
that fits a model to the dataset.
Parameters of the \texttt{KMeans} estimator are listed as follows:
\begin{cli}
  >>> from pyspark.ml.clustering import KMeans
  >>> KMeans().explainParams()
\end{cli}
Having loaded the dataset as in the previous section, the KMeans
estimator is used as follows:
\begin{cli}
  >>> kmeans = KMeans(k=2)
  >>> model=kmeans.fit(dataset)
\end{cli}
The resulting model essentially contains the cluster centroids:
\begin{cli}
  >>> print model.clusterCenters()
\end{cli}
Finally, a model can be applied to a dataset as follows:
\begin{cli}
  >>> classified_dataset=model.transform(dataset)
  >>> classified_dataset.show()
\end{cli}

The complete example is available here:
\pythoncode{mllib/clustering.py}\\
See also
\href{https://github.com/apache/spark/blob/master/examples/src/main/python/ml/kmeans_example.py}{example
  in Spark repository}.

\section{Collaborative filtering}

MLlib implements latent-factor-based collaborative filtering using the
alternate least-square method. Using MLlib, you can implement a
recommendation system with a few lines of code only.  The example
below, extracted from the Spark documentation, loads a sample of the
MovieLens dataset, parses it as
\href{http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row}{DataFrame
  Rows} and creates a DataFrame from these Rows:
\begin{cli}
  >>> from pyspark.sql import Row
  >>> example_file_path=os.path.join(os.environ['SPARK_HOME'],\
      "data/mllib/als/sample_movielens_ratings.txt")
  >>> lines = spark.read.text("file://"+example_file_path).rdd
  >>> parts = lines.map(lambda row: row.value.split("::"))
  >>> ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),
                                     rating=float(p[2]), timestamp=long(p[3])))
  >>> ratings = spark.createDataFrame(ratingsRDD)
\end{cli}
For evaluation purposes, the DataFrame is then split in a training and
a testing (evaluation) set:
\begin{cli}
  >>> (training, test) = ratings.randomSplit([0.8, 0.2])
\end{cli}
The recommendation model is then built as follows:
\begin{cli}
  >>> from pyspark.ml.recommendation import ALS
  >>>  als = ALS(maxIter=5, regParam=0.01, userCol="userId", itemCol="movieId", ratingCol="rating")
  >>> model = als.fit(training)
\end{cli}
And finally its RMSE can be evaluated on the test data:
\begin{cli}
  >>> from pyspark.ml.evaluation import RegressionEvaluator
  >>> predictions = model.transform(test)
  >>> evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating",
                                predictionCol="prediction")
  >>> rmse = evaluator.evaluate(predictions)
\end{cli}
The complete example is available in the \href{https://github.com/apache/spark/blob/master/examples/src/main/python/ml/als_example.py}{Spark documentation}.

\section{Frequent itemsets mining}

MLlib contains a few algorithms for frequent itemsets mining, however,
they are currently available only in the RDD-based API which has
entered maintenance mode since Spark 2.0. In the following code
snippets, note the imports from \texttt{pyspark.mllib} while
\texttt{pyspark.ml} is used in the DataFrame API. Besides, not all the
algorithms have Python bindings.

%% In the RDD API, a Spark context must first be created:
%% \begin{cli}
%%   >>> from pyspark import SparkContext, SparkConf
%%   >>> conf = SparkConf().setAppName("frequent itemsets").setMaster("local")
%%   >>> sc = SparkContext(conf=conf)
%% \end{cli}
Here is how to load and parse test data:
\begin{cli}
  >>> example_file_path=os.path.join(os.environ['SPARK_HOME'],"data/mllib/sample_fpgrowth.txt")
  >>> data = sc.textFile("file://"+example_file_path)
  >>> transactions = data.map(lambda line: line.strip().split(' '))
\end{cli}
The \href{http://dx.doi.org/10.1145/335191.335372}{FP-growth}
algorithm is available in Python:
\begin{cli}
  >>> from pyspark.mllib.fpm import FPGrowth
  >>> model  = FPGrowth.train(transactions,minSupport=0.2, numPartitions=10)
  >>> result = model.freqItemsets().collect()
  >>> for fi in result:
  ...   print(fi)
\end{cli}
The complete example is available in the
\href{https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/fpgrowth_example.py}{Spark
  documentation}.

\end{document}


