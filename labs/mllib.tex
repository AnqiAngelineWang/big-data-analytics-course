\documentclass[11pt]{article}

% Packages
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}

% Colors
\definecolor{mygray}{RGB}{235,235,235}

% Margins
\setlength{\topmargin}{-2cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{24cm}
\setlength{\evensidemargin}{0cm}
\setlength{\oddsidemargin}{0cm}

% Fix link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}


% Code listings
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  commentstyle=\color{magenta}\ttfamily,
  morecomment=[l][\color{magenta}]{\#}
}


\lstnewenvironment{cli}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=bash,
                      backgroundcolor=\color{mygray}
                  }}
{}

\lstnewenvironment{xml}
                  {\footnotesize
                    \lstset{columns=fullflexible,
                      language=XML,
                      backgroundcolor=\color{Salmon},
                      morekeywords={property,name,value,description,configuration}
                  }}
{}

\newcommand{\bashcode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {
           \href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
                {(\underline{Link to file})
         }} \hfill
         \lstset{language=bash,
           columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\pythoncode}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{YellowGreen}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{language=java,
           columns=fullflexible,
           backgroundcolor=\color{YellowGreen}}
%  \vspace*{-0.3cm}
%  \lstinputlisting{#1}
  \end{footnotesize}
}

\newcommand{\textfile}[1]{
  \begin{footnotesize}
  \par
  \hfill \colorbox{SkyBlue}
         {\href{https://github.com/glatard/big-data-analytics-labs/raw/master/labs/#1}
           {(\underline{Link to file})
         }} \hfill
         \lstset{columns=fullflexible,
           backgroundcolor=\color{SkyBlue}}
  \vspace*{-0.3cm}
  \lstinputlisting{#1}
  \end{footnotesize}
}


% Notes and TODOs              
\newcommand{\postit}[1]{%
  \noindent
  \fcolorbox{red}{yellow}{%
    \begin{minipage}{5cm}
      #1
    \end{minipage}
   }
}

\title{\textsc{Big Data Analytics (SOEN 498/691)} \\ Laboratory sessions}

\author{Tristan Glatard\\Department of Computer Science and Software Engineering\\Concordia University, Montreal\\\href{mailto:tristan.glatard@concordia.ca}{tristan.glatard@concordia.ca}}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\part{MLlib}

\section{Introduction}

MLlib (Machine-Learning library) is a library in
\href{http://spark.apache.org}{Apache Spark} that contains
implementations of several algorithms seen in the lecture. In this
session, we will run through basic examples related to clustering,
recommendation systems and the mining of frequent itemsets.  Most of
the material in this document is taken from the
\href{http://spark.apache.org/docs/latest/ml-guide.html}{MLlib
  guide}. You are encouraged to further explore this guide should you
need more details about MLlib. For installation instructions and an
introduction to Apache Spark, please refer to the
\href{https://github.com/glatard/big-data-analytics-course/releases/download/0.8/spark.pdf}{previous
  lab session}. Warning: MLlib is still under active development and
backward compatibility even between minor versions is not
ensured. Here we use version 2.1.0. 

\href{Pyspark documentation}{http://spark.apache.org/docs/latest/api/python/index.html}

\section{The DataFrame API}

This section is adapted from the
\href{http://spark.apache.org/docs/latest/sql-programming-guide.html}{Spark
  DataFrame Guide}.

In the introduction session we presented how Resilient Distributed
Datasets are used to define parallel operations on datasets. MLlib
uses another kind of data objects, called DataFrame. A DataFrame
consists of named columns. It is conceptually equivalent to a table in
a database.

Before using the DataFrame API, a Spark session must be created:
\begin{cli}
  >>> spark = SparkSession.builder.appName("Lab session").getOrCreate()
\end{cli}

A DataFrame can be created from an RDD or from a file. For instance,
here is how to read a DataFrame from a
\href{libSVM}{https://www.csie.ntu.edu.tw/~cjlin/libsvm/} file:
\begin{cli}
  >>> import os
  >>> spark_home = os.environ['SPARK_HOME']
  >>> dataset = spark.read.format("libsvm").load("file://''+os.join(spark_home,''data/mllib/sample_kmeans_data.txt")
\end{cli}
DataFrame objects can then be inspected and queried as a database relation:
\begin{cli}
>>> dataset.show()
>>> dataset.select("label").show()
>>> dataset.filter(dataset["label"]==1).show()
\end{cli}
See more details in the \href{http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame}{PySpark documentation}.

\section{Clustering}

MLlib includes an enhanced version of kmeans called kmeans||.

SVM format.


\pythoncode(mllib/clustering.py}

\section{Collaborative filtering}



\section{Frequent itemsets mining}

\end{document}


